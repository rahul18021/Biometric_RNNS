{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition (Part 1: Data/Image Pre-Processing)\n",
    "\n",
    "In this tutorial, we are going to do some data preprocessing and go over some standard steps such that the data is prepared to be used as input to the model/network for face recognition. We will use face images of a random subset of 10 celebrities, out of the 2,622.\n",
    "\n",
    "Let's go through these step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "DATA_ROOT = \"/tmp/data/lab3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. GETTING THE DATA\n",
    "\n",
    "The dataset that we shall be using is a subset of the [VGG-Face](http://www.robots.ox.ac.uk/~vgg/data/vgg_face/) dataset. It consists of 20 images each of 10 Indian celebrities. We have already pre-processed it and it is availabale in a directory called vgg_face_indian_dataset.\n",
    "\n",
    "**There is NO need to download any files from the [VGG-Face] link**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "celebs = [\"A.R._Rahman\", \"Aamir_Khan\", \"Amitabh_Bachchan\", \"A_P_J_Abdul_Kalam\", \"Kamal_Hassan\", \"Madhuri_Dixit\", \"Mahendra_Singh_Dhoni\", \"Preity_Zinta\", \"Vidya_Balan\", \"Virat_Kohli\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understanding the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see a raw image in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directory names\n",
    "dataset_dir = DATA_ROOT+'/vgg_face_indian_dataset'\n",
    "raw_images_dir = DATA_ROOT+'/vgg_face_indian_dataset/raw'\n",
    "face_images_dir = DATA_ROOT+'/vgg_face_indian_dataset/faces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an image in the \"raw\" directory\n",
    "example_raw_image = cv2.imread(os.path.join(raw_images_dir, \"A.R._Rahman_01.jpg\"))\n",
    "# Show it\n",
    "plt.imshow(cv2.cvtColor(example_raw_image, cv2.COLOR_BGR2RGB)) # Ignore \"cv2.COLOR_BGR2RGB\"\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, 20 images each of 10 celebrities.\n",
    "\n",
    "The bounding box of the faces in these image have been found, and the images have been rotated to make the faces straight, and then converted to grayscale. These grayscale images have been saved in the **vgg_face_indian_dataset/faces** directory.\n",
    "\n",
    "Let us read all the images in the faces dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read all images in the \"faces\" directory\n",
    "images = []\n",
    "for celeb in celebs:\n",
    "    images.append([])\n",
    "    # This code inside the loop constructs the filename as Name_xx.jpg for each name\n",
    "    # xx runs from 01 - 20\n",
    "    for i in range(1, 21):\n",
    "        filenum = \"_{0:02d}\".format(i)\n",
    "        filename = face_images_dir + \"/\" + celeb + filenum + \".jpg\"\n",
    "        images[-1].append(cv2.imread(filename, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$images$ is a list containing 10 lists. Each of those 10 lists contains images of a particular celebrity.\n",
    "\n",
    "Each of the 10 lists contains 20 images, i.e. 20 images per celebrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(images), type(images))\n",
    "# Finding the length of each list in \"images\"\n",
    "print(len(images[0]), type(images[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these 20 images per celebrity is a \"numpy array\". These images are of variable size. For example, the first two images of A.R. Rahman are of sizes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_rahman_index = 0\n",
    "print(images[ar_rahman_index][0].shape, images[ar_rahman_index][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, all the images are of different sizes. Let us visualize one image per class (celebrity) in the dataset, and note the sizes of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to plot 1 image per class in the given dataset\n",
    "def plot_one_per_class(images):\n",
    "    # Number of images ot be plotted\n",
    "    N = 10    # Plot one image of each class (#classes=10)\n",
    "    #each plot size\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # For each class\n",
    "    for i, celeb in enumerate(celebs):\n",
    "        # Make subplot\n",
    "        plt.subplot(2, 5, i+1)    # plt.subplot(n_rows, n_columns, image_position_in_plot)\n",
    "\n",
    "        # Plot the 0th image for each class (in grayscale)\n",
    "        example_face_image = images[i][0]\n",
    "        plt.imshow(example_face_image, cmap=\"gray\")\n",
    "\n",
    "        # Turn off axis lines\n",
    "        plt.axis(\"off\")\n",
    "        # (Optional) Write the size of the image as its title\n",
    "        plt.title(\"size=\"+str(example_face_image.shape), size=18)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_one_per_class(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Splitting the dataset into \"train\", \"val\" and \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring class balance\n",
    "\n",
    "It is important to maintain equal/similar number of images per celebrity in the train and val datasets. This ensures that the model and the calculated accuracies are not biased towards any class. We know that we have 20 images per celebrity and we divide the dataset such that we have 60% of it as training and 20% as validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train_per_class = 12\n",
    "n_val_per_class = 4\n",
    "n_test_per_class = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these values, let us make the train, val and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the train, val, and test sets\n",
    "images_train = []\n",
    "images_val = []\n",
    "images_test = []\n",
    "# For each celebrity\n",
    "for i, celeb in enumerate(celebs):\n",
    "    \n",
    "    # Add a new empty list item\n",
    "    images_train.append([])\n",
    "    images_val.append([])\n",
    "    images_test.append([])\n",
    "    \n",
    "    # Add the specified number of images to the train set\n",
    "    for train_iter in range(0, n_train_per_class):\n",
    "        images_train[-1].append(images[i][train_iter])\n",
    "    \n",
    "    for val_iter in range(n_train_per_class, n_train_per_class + n_val_per_class):\n",
    "        images_val[-1].append(images[i][val_iter])\n",
    "    \n",
    "    for test_iter in range(n_train_per_class + n_val_per_class, n_train_per_class + n_val_per_class + n_test_per_class):\n",
    "        images_test[-1].append(images[i][test_iter])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE MANIPULATIONS\n",
    "\n",
    "In order to train a model, the images have to be manipulated so that they have similar properties. We shall see these manipulation tasks below.\n",
    "\n",
    "We will transform the training set first, step-by-step and then use it as a function and apply on other sets as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the images can be of different sizes, we need to resize all the images to a constant size (224 x 224)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resize images\n",
    "resized_images_train = np.zeros((10, int(n_train_per_class), 224, 224)) # 10 classes, 12 images per class, each image of size (224, 224)\n",
    "for i in range(len(celebs)):\n",
    "    for j in range(int(n_train_per_class)):\n",
    "        image = images_train[i][j]\n",
    "        resized_image = cv2.resize(image, (224, 224))\n",
    "        resized_images_train[i][j] = resized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot one **resized_image** of each class to check if resizing worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_per_class(resized_images_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Normalization\n",
    "\n",
    "We know that image pixel values range between 0 and 255. As an example, let us see this range in the 4th image of the class= \"Madhuri Dixit\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "madhuri_dixit_index = 5\n",
    "example_image = resized_images_train[madhuri_dixit_index][4]\n",
    "# in plt.imshow - we specify that the min and max possible values in the image as 0 and 255\n",
    "# as otherwise plt.imshow() normalizes it automatically\n",
    "plt.imshow(example_image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "# Printing the minimum and maximum values of the image\n",
    "print(resized_images_train[madhuri_dixit_index][4].min(), resized_images_train[madhuri_dixit_index][4].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the maximum pixel value in this image is 171 which also explains why this image appears dark. This thus makes it important to normalize the images first before building a model over them. As each image depending on the capture setting can have a different range of values, we need to mormalize the image such that its pixel values are stretched out in the range of 0-255.\n",
    "\n",
    "Let us scale the pixels values in **each** image in the dataset such that the minimum pixel value within the image becomes 0, and the maximum becomes 255. This way, we are ensuring that the full range of values are being covered in the image. This should result in the above image becoming _brighter_. This is called MinMax Scaling/Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MinMax Scaling of images\n",
    "minmax_scaled_images_train = np.zeros((10, int(n_train_per_class), 224, 224)) # 10 celebrities, some images per celebrity, each image of size (224, 224)\n",
    "for i in range(len(celebs)):\n",
    "    for j in range(int(n_train_per_class)):\n",
    "        resized_image = resized_images_train[i][j]\n",
    "        # normalize image using minmax scaling\n",
    "        minmax_scaled_image = (resized_image - np.min(resized_image))/(np.max(resized_image) - np.min(resized_image))*255\n",
    "        minmax_scaled_images_train[i][j] = minmax_scaled_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if this worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing image before and after min-max scaling\n",
    "plt.subplot(121)\n",
    "plt.imshow(resized_images_train[madhuri_dixit_index][4], cmap='gray', vmin=0, vmax=255)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(minmax_scaled_images_train[madhuri_dixit_index][4], cmap='gray', vmin=0, vmax=255)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "# Printing the minimum and maximum values of the image\n",
    "print(minmax_scaled_images_train[madhuri_dixit_index][4].min(), minmax_scaled_images_train[madhuri_dixit_index][4].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Changing the input dimensions (depends on model) \n",
    "\n",
    "Till now, we maintained the 1st dimension as iterating through celebrities, and the 2nd dimension as iterating throught the images of each celebrity. This was done only for illustrative purposes.\n",
    "\n",
    "$$shape(minmax\\_scaled\\_images\\_train) = num\\_class\\times n\\_train\\_per\\_class\\times224\\times224$$.\n",
    "\n",
    "But, for EigenFaces the input dimensions is as follows:\n",
    "\n",
    "(1) the first dimension is iterating through all the samples.\n",
    "\n",
    "(2) the second dimension is the features (or data values).\n",
    "\n",
    "Therefore, let us combine the first & second dimensions as they are the data samples. Similarly, we combine the last 2 dimensions as they represent the features (the data values) resulting in the new shape $$(num\\_class * n\\_train\\_per\\_class)\\times(224 * 224)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_images = np.reshape(minmax_scaled_images_train, (10*int(n_train_per_class), 224* 224))\n",
    "print(reshaped_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subtracting $mean\\_image$\n",
    "\n",
    "Since we are interested in the difference between the faces, let’s subtract the characteristics which are common between them. The common characteristic of each pixel value is its mean among all the training images. Thus, let us find the **mean_image**, and subtract it from all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean image\n",
    "mean_image = np.mean(reshaped_images, axis=0)\n",
    "print(mean_image.shape) # should be 51076, i.e., 224*224\n",
    "final_images_train = reshaped_images - mean_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing mean image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(mean_image, (224, 224)), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the mean image consists of a pair of eyes, a nose, a mouth, etc., at the right places. This is possible since all the face images in the dataset were oriented and aligned to each other.\n",
    "\n",
    "Had the images not been aligned, the mean image would have looked like this:\n",
    "\n",
    "<img src=DATA_ROOT+\"/vgg_face_indian_dataset/hazy_mean_image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDATION AND TEST DATA\n",
    "\n",
    "## - Image manipulations for validation and test data\n",
    "\n",
    "The same image manipulations must be carried out on the validation and test images. As a part of exercise, complete a function that performs all the operations step-by-step.\n",
    "\n",
    "**Note that the mean image is calculated only on the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_data(images, n_images_per_class, mean_image):\n",
    "    \n",
    "    #### Your Code Here\n",
    "    \n",
    "    return final_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this function to find the final images of val and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding final images for val and test\n",
    "final_images_val = transform_data(images_val, n_val_per_class, mean_image)\n",
    "final_images_test = transform_data(images_test, n_test_per_class, mean_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABELS\n",
    "\n",
    "Let us make the labels for train, val and test. Each of the 10 celebrities shall be associated with a number among {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_train = np.array([[i]*n_train_per_class for i in range(10)]).flatten()\n",
    "labels_val = np.array([[i]*n_val_per_class for i in range(10)]).flatten()\n",
    "labels_test = np.array([[i]*n_test_per_class for i in range(10)]).flatten()\n",
    "print(labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE DATA\n",
    "\n",
    "Let us save the data as \"data.npz\" format, so that we can load it in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the train, val, and test data and labels, and the mean image\n",
    "np.savez(\"data\",\n",
    "         data_train=final_images_train, labels_train=labels_train,\n",
    "         data_val=final_images_val, labels_val=labels_val,\n",
    "         data_test=final_images_test, labels_test=labels_test,\n",
    "         mean_image=mean_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have prepared the data, lets move to Part 2 of this exercise\n",
    "\n",
    "Note that we have already saved a data.npz file in the data folder in case you want to directly jump to the next experiment without completing the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Write code for the function **transform_data()** which performs all the image manipulations discussed in a single function that can be used to perform the same on test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
