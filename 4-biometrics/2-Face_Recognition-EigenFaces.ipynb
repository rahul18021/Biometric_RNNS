{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition (Part 2: EigenFaces)\n",
    "\n",
    "In this tutorial, we are going to use EigenFaces to recognize the faces of celebrities. We will use face images of a random subset of 10 celebrities, out of the 2,622.\n",
    "\n",
    "Let's go through these step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "In this experiment, we shall see:\n",
    "\n",
    "- **1. FEATURE EXTRACTION**\n",
    "    - extract eigenvalues and eigenvectors, and choose the best N principal components as features\n",
    "\n",
    "\n",
    "- **2. TRAINING AND PREDICTION**:\n",
    "    - We will train a Naive Bayes classifier on the extracted features and evaluate its performance\n",
    "\n",
    "\n",
    "- **3. COMPARISON OF ALL ACCURACIES**: compare training, validation and testing accuracies.\n",
    "\n",
    "Let us go through these step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "DATA_ROOT = \"/tmp/data/lab3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap of Part 1\n",
    "\n",
    "In Part 1, we understood the data, and split it into train, val, and test. We then manipulated it so that the data is of uniform size, normalized, and mean subtracted. (Not that these operations can be performed on any data, not just images.)\n",
    "\n",
    "We then saved the final datasets as \"data.npz\" file. Let us load them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data = np.load(DATA_ROOT+\"/data.npz\")\n",
    "\n",
    "# Getting train, val and test data and labels, and the mean_image\n",
    "data_train = data[\"data_train\"]\n",
    "labels_train = data[\"labels_train\"]\n",
    "data_val = data[\"data_val\"]\n",
    "labels_val = data[\"labels_val\"]\n",
    "data_test = data[\"data_test\"]\n",
    "labels_test = data[\"labels_test\"]\n",
    "mean_image = data[\"mean_image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. FEATURE EXTRACTION (Eigen Faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First $N$ Principal Components with maximum eigenvalues\n",
    "\n",
    "As we have learnt in the lecture, PCA finds the set of orthonormal vectors which best\n",
    "describe the distribution of the underlying dataset. In the given dataset, we have $n$\n",
    "images of size $K \\times K$. (We know that $K = 224$, and $n = 120$ in the training set)\n",
    "\n",
    "Let us now see how the PCA features are extracted:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Find eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_eigenvalues_and_eigenvectors(A):\n",
    "    L = 1 / len(A.T) * np.dot(A, A.T)\n",
    "    e, u = np.linalg.eig(L)\n",
    "    w = e\n",
    "    v = np.dot(A.T, u)\n",
    "    return w, v\n",
    "\n",
    "eigenvalues, eigenvectors = find_eigenvalues_and_eigenvectors(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Reordering, normalizing\n",
    "\n",
    "- We need to reorder them so that they are in descending order of eigenvalues,\n",
    "- Also, normalize the eigenvectors so that their norms are 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the required order of indices to make decreasing order of eigenvalue\n",
    "sort_index = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "# Use the calculated order of indices to reorder eigenvalues and eigenvectors\n",
    "eigenvalues = eigenvalues[sort_index]\n",
    "eigenvectors = eigenvectors[:, sort_index]\n",
    "\n",
    "# NORMALIZE\n",
    "eigenvectors = eigenvectors / np.linalg.norm(eigenvectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Eigenfaces\n",
    "\n",
    "The eigenvectors thus found are called eigenfaces (because we found the eigenvectors of faces...).\n",
    "\n",
    "Since an eigenvector is of dimension ($K^2$,), it can be reshaped to $(K, K)$ and displayed as an image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the first 50 eigenfaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of eigenfaces to be plotted\n",
    "N = 50\n",
    "plt.figure(figsize=(10, 2*(N+5)//5))\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    # Make a subplot\n",
    "    plt.subplot((N + 5)//5, 5, i+1)\n",
    "    \n",
    "    # Plot the eigenface, after reshaping it to (224, 224)\n",
    "    # Remember eigenfaces are **columns** in the matrix\n",
    "    plt.imshow(np.reshape(eigenvectors[:, i], (224, 224)), cmap='gray')    \n",
    "    # Turn off axis lines\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Computing good value for $N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the given dataset, there are as many eigenvectors as the number of training examples. This can be verified by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each column is an eigenvector, there are 120 eigenvectors, each of 50176 dimensions. But usually, a smaller number $N$ of eigenvectors is chosen as a basis to make feature vectors. This forms the main point of PCA as we reduce the dimension of our representation while trying to capture the maximum variance in the data at the same time.\n",
    "\n",
    "To decide the on the number $N$, i.e. the number of most important eigenvectors to keep as the basis, ((the cumulative sum of eigenvalues (assuming they are in decreasing order) divided by the total sum of eigenvalues)), vs. (( the number of eigenvalues considered ($M$) )) is plotted.\n",
    "\n",
    "This plot basically tells us the fraction of total variance retained ($r$) vs. the number of eigenvalues considered ($M$) to retain that variance in data. This way, the plot gives a good understanding of the point of diminishing returns, i.e. the point where little variance is retained by retaining additional eigenvalues.\n",
    "\n",
    "This can be understood by the following equation:\n",
    "\n",
    "$$r = \\frac{\\sum_{k=1}^{M}\\lambda_k}{\\sum_{k=1}^{n}\\lambda_k},\\ \\ \\ \\  M <= n$$\n",
    "\n",
    "Plotting $r$ vs $M$ shall give a good idea of the impact of varying $M$ on $r$ and help you decide a good value of $N$ depending on how much variance (~80% is a popular choice) you want to retain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a part of **exercise**, we leave it to you to come to a conclusion for what will be a good value for N.\n",
    "Let us choose $N = 20$ to continue with our experiment here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we are choosing only $N$ **principal components**. In other words, we are choosing those $N (<< n(=120))$ eigenvectors that are most important in faces. We can look at the plots of the eigenfaces to see what sort of information we are choosing.\n",
    "\n",
    "Let us note the first N principal components, i.e. the first N eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_vectors = eigenvectors[:, :N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Finding features using first $N$ Principal Components\n",
    "\n",
    "\n",
    "Since we are using the most important eigenfaces as the _basis_ vectors, we need to project the data into these basis components to find the relevant features. We do this by finding the dot product of the data maxtrix and the matrix of the most important eigenvectors.\n",
    "\n",
    "We know that the data (`data_train`, `data_val`, `data_test`) is of shape $n \\times K^2$. We also know that the `pca_vectors` matrix is of shape $K^2 \\times N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_features_train = np.dot(data_train, pca_vectors)\n",
    "pca_features_val = np.dot(data_val, pca_vectors)\n",
    "pca_features_test = np.dot(data_test, pca_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shapes of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_features_train.shape, pca_features_val.shape, pca_features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can see that we have transformed our data from $n \\times K^2$ to $n \\times N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training and Prediction\n",
    "\n",
    "We will try a Naive Bayes Classifier for the multi class classification problem at hand. We have PCA features at our disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes algorithm\n",
    "We have features and the labels present in the dataset. Now we need to learn naive bayes classifier for this dataset. We will go through a quick mathematical formulation of the Naive Bayes below.\n",
    "\n",
    "The Naive Bayes Equation can be written as:\n",
    "\n",
    "$$ P(Y = y|X) = P(X|Y = y)*P(Y = y) $$\n",
    "\n",
    "where $X_i$ is a feature and $Y$ is the set of all labels. Therefore, we need to find the following 2 terms: $P(Y = y)$ called the **prior** and $P(X|Y = y)$ called the **likelihood**\n",
    "\n",
    "### Calculate Prior $P(Y)$\n",
    "This is also called the class probability. $P(Y)$ or $P(Y = y)$ is simply the fraction of the elements present in a class.\n",
    "\n",
    "### Calculating Likelihood $P(X|Y = y)$ or $P(X_{i}|Y = y)$\n",
    "\n",
    "For each feature $X_{i}$, we assume that they are uncorrelated and thus we can write,\n",
    "\n",
    "$$ P(X|Y = y) = \\Pi_{i} P(X_{i} | Y = y) $$\n",
    "\n",
    "We wish to estimate $P(X_{i} | Y = y)$ for each $X_{i}$. Thus for samples corresponding to each class label $y$, we need to estimate a probability distribution. This is done by modelling the likelihood as a normal distribution (also called gaussian distribution),\n",
    "\n",
    "$$ P(X_{i}|Y = y) = \\frac{1}{\\sqrt{2\\pi v_{y, i}^{2}}} exp( - \\frac{ (X_{i} - m_{y, i})^{2} }{2v_{y, i}^{2}} ) $$ \n",
    "\n",
    "As our features are real values, we estimate a normal distribution **corresponding to each feature** using all training samples per class.\n",
    "\n",
    "Thus, we need to calculate mean ($ m_{y, i} $) and standard deviation ($ v_{y, i} $).\n",
    "\n",
    "Once we've the mean and standard deviation for each feature and the prior probability for each class, we can use the above formulation to get the required probability $ P(X|Y = y) $ also called the **posterior probability**.\n",
    "$$ P(X|Y = y) = (\\Pi_{i}\\frac{1}{\\sqrt{2\\pi v_{y, i}^{2}}} exp( - \\frac{ (X_{i} - m_{y, i})^{2} }{2v_{y, i}^{2}} ))*P(Y = y) $$\n",
    "\n",
    "We code the classifier using the functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function to calculate prior for each class and the mean, std deviation for each feature\n",
    "def get_prior_mean_dev(train_features, train_labels):\n",
    "\n",
    "    classes, counts = np.unique(train_labels, return_counts=True)\n",
    "    number_of_classes = len(classes)\n",
    "    number_of_features = train_features.shape[1]\n",
    "\n",
    "    # Prior\n",
    "    prior = []\n",
    "    for c, class_label in enumerate(classes):\n",
    "        prior.append(counts[c]/len(train_labels))\n",
    "\n",
    "    # Calculate the mean and variance per feature dimension here \n",
    "    # from the training set from samples belonging to each class label.\n",
    "    means = np.zeros((number_of_features, number_of_classes)) # every feature, for each class\n",
    "    std_dev = np.zeros((number_of_features, number_of_classes)) # every feature, for each class\n",
    "    # For each class\n",
    "    for c, class_label in enumerate(classes): # selecting a class 'y'\n",
    "        class_rows = train_features[np.where(train_labels == class_label)[0], :]    # get all samples belonging to 'class_label'\n",
    "        # For each feature\n",
    "        for f in range(number_of_features):\n",
    "            means[f, c] = np.mean(class_rows[:, f])\n",
    "            std_dev[f, c] = np.std(class_rows[:, f])\n",
    "\n",
    "    NB_classifier = {}\n",
    "    NB_classifier['prior'] = prior\n",
    "    NB_classifier['means'] = means\n",
    "    NB_classifier['std_dev'] = std_dev\n",
    "\n",
    "    return NB_classifier\n",
    "\n",
    "## function to draw from the gaussian distribution for each feature \n",
    "def gaussian(x, m, v):\n",
    "    return np.sqrt(1.0 / 2*np.pi) / v * np.exp(-0.5*(((x - m)/v)**2) )\n",
    "\n",
    "## function to calculate likelihood based on features and their mean and deviation\n",
    "def get_likelihood(features, means, std_dev):\n",
    "\n",
    "    number_of_features, number_of_classes = means.shape\n",
    "\n",
    "    # Feature probabilities\n",
    "    # feature_probabilities = gaussian(tiled_features, means, std_dev) # get the probability\n",
    "    feature_probabilities = np.zeros((number_of_features, number_of_classes))\n",
    "    for f in range(number_of_features):\n",
    "        for c in range(number_of_classes):\n",
    "            feature_probabilities[f, c] = gaussian(features[f], means[f, c], std_dev[f, c])\n",
    "\n",
    "    # Multiply all features' probabilities to get likelihood\n",
    "    # Likelihood of each class\n",
    "    likelihood = np.zeros((number_of_classes))\n",
    "    for c in range(number_of_classes):\n",
    "        likelihood[c] = np.prod(feature_probabilities[np.nonzero(feature_probabilities[:, c]), c]) # mutliply for each feature 'Xi'\n",
    "    # likelihood = np.prod(feature_probabilities, axis=0)\n",
    "\n",
    "    return likelihood\n",
    "\n",
    "## function to get the classification accuracy based on Naive Bayes Classifier\n",
    "def naive_bayes_classify(NB_classifier, test_features, test_labels):\n",
    "\n",
    "    # NB_classifier is a dict with keys \"prior\", \"means\" and \"std_dev\"\n",
    "    prior = NB_classifier[\"prior\"]\n",
    "    means = NB_classifier[\"means\"]\n",
    "    std_dev = NB_classifier[\"std_dev\"]\n",
    "\n",
    "    if len(test_features.shape) == 1:\n",
    "        test_features = np.reshape(test_features, (1, len(test_features)))\n",
    "\n",
    "    predicted_labels = []\n",
    "\n",
    "    for t, test_sample_features in enumerate(test_features):\n",
    "        # print(\"Progress: {0:0.04f}\".format((t+1)/len(test_features)), end=\"\\r\")\n",
    "\n",
    "        # Get the likelihood of the test samples belonging to each class\n",
    "        likelihood = get_likelihood(test_sample_features, means, std_dev)\n",
    "\n",
    "        # Calculate the approximate posterior = likelihood * prior\n",
    "        approx_posterior = [np.asscalar(x*y) for x,y in zip(likelihood, prior)]\n",
    "        #approx because of missing P(X) (constant) in the denominator\n",
    "\n",
    "        # Make the prediction as that class with the maximum approximate posterior\n",
    "        predicted_labels.append(np.argmax(approx_posterior))\n",
    "\n",
    "    return np.mean(predicted_labels == test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_clf = get_prior_mean_dev(pca_features_train, labels_train)\n",
    "NB_pca_test_acc = naive_bayes_classify( NB_clf, pca_features_test, labels_test)\n",
    "print(NB_pca_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises\n",
    "2. Find a good value for **N** (the number of eigenvectors to be used) by plotting $r$ vs $M$ curve and visualizing the number of eignevectors with a certain minimum variance (~80% is a popular choice) you want to retain.\n",
    "3. Find both training and testing accuracy over the given dataset. Study the effect of changing train/test ratio (60% and 20% chosen initially) on the accuracy. Does the classifier overfit at large train:test ratio?\n",
    "4. Instead of a Naive Bayes Classifier, can you train a small NN/CNN for this classification task? If yes, evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
