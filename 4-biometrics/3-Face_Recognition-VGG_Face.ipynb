{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition (Part 3: VGG-Face)\n",
    "\n",
    "In this tutorial, we are going to use a pre-trained VGG-Face network to recognize the faces of celebrities. The VGG-Face is a deep-CNN which has been trained using Softmax loss to recognize faces of 2,622 celebrity identities. We will use face images of a random subset of 10 celebrities, out of the 2,622.\n",
    "\n",
    "Let's go through these step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "In this experiment, we shall see:\n",
    "\n",
    "- **1. VGG-Face (deep CNN) for Face Recognition**\n",
    "    - using a pretrained deep CNN for Face Recognition\n",
    "\n",
    "\n",
    "- **2. Feature Extraction**:\n",
    "    - extracting deep features \n",
    "    \n",
    "Let us go through these step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.serialization import load_lua\n",
    "from torch.legacy import nn\n",
    "\n",
    "DATA_ROOT = \"/tmp/data/lab3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap of Part 1 (Preprocessing)\n",
    "\n",
    "In Part 1, we understood the data, and split it into train, val, and test. We then manipulated it so that the data is of uniform size, normalized, and mean subtracted. (Not that these operations can be performed on any data, not just images). We then saved the final datasets as \"data.npz\" file.\n",
    "\n",
    "However, we will not be using that same dataset here. Reasons:\n",
    "- The old dataset has just 120 training images (12 per class) which is very less for a deep-CNN architecture\n",
    "- The VGG-Face network takes RGB images as input\n",
    "- the input dimension should be: $(num\\_class * n\\_train\\_per\\_class)\\times3\\times224\\times224$\n",
    "- Other preprocessing steps like resizing, normalization, mean subtraction will still be employed though.\n",
    "\n",
    "So, here we will load a small test dataset (**test_data_vgg_face.npz**) which has 10 random celebs (1 image per celeb) and perform the necessary preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(DATA_ROOT+\"/test_data_vgg_face.npz\")\n",
    "image = data[\"images\"][0]\n",
    "label = data[\"labels\"][0]\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(label)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(\"all images in test data = \",data[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, can we simply perform a resize operation on our input image and convert it to the appropriate size? Performing a simple resize operation on arbitrary sized input images may result in a change in the aspect ratio, thereby making the face image look distorted.\n",
    "\n",
    "To avoid that, we perform the following sequence of operations\n",
    "- Resize the image such that the smaller dimension (out of height and width) is 256 and the aspect ratio remains the same.\n",
    "- Crop a 224 $\\times$ 224 region from the center of the resized image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_resize(inputImg, targetH, targetW):\n",
    "    inpH, inpW = inputImg.shape[0], inputImg.shape[1]\n",
    "    # re-scale the smaller dim (among width, height) to refSize\n",
    "    refSize = 256\n",
    "    if inpW < inpH:\n",
    "        resizedImg = cv2.resize(inputImg, (refSize, int(refSize*inpH/inpW)))\n",
    "    else:\n",
    "        resizedImg = cv2.resize(inputImg, (int(refSize*inpW/inpH), refSize))\n",
    "\n",
    "    # center-crop\n",
    "    iH, iW = resizedImg.shape[0], resizedImg.shape[1]\n",
    "    anchorH, anchorW = int(math.ceil((iH - targetH)/2)), int(math.ceil((iW - targetW) / 2))\n",
    "    croppedImg = resizedImg[anchorH:anchorH+targetH, anchorW:anchorW+targetW]\n",
    "    return croppedImg\n",
    "\n",
    "\n",
    "print(\"old dimensions:\",image.shape)\n",
    "naive_resized_image = cv2.resize(image, (224, 224))\n",
    "opt_resized_image = optimized_resize(image, 224, 224)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(131)\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"original image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(cv2.cvtColor(naive_resized_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"naive resize\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(cv2.cvtColor(opt_resized_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"resize+crop\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "print(\"dimension after resizing:\", opt_resized_image.shape)\n",
    "\n",
    "## change dimension from 224x224x3 to 3x224x224\n",
    "r, g, b = opt_resized_image[:, :, 0], opt_resized_image[:, :, 1], opt_resized_image[:, :, 2]\n",
    "final_image = np.empty([3, opt_resized_image.shape[0], opt_resized_image.shape[1]])\n",
    "final_image[0], final_image[1], final_image[2] = r, g, b\n",
    "print(\"new_dimensions:\",final_image.shape)\n",
    "\n",
    "## Calculated mean from trained data (available for VGG-Face)\n",
    "trainingMean = [129.1863, 104.7624, 93.5940]\n",
    "for i in range(3): final_image[i] = final_image[i] - trainingMean[i]\n",
    "\n",
    "print(final_image.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. VGG-Face (Deep CNN for face recognition)\n",
    "\n",
    "Here, we will use a pretrained VGG Face model to generate predictions for the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained VGG-Face network\n",
    "vggFace = load_lua(DATA_ROOT+\"/VGG_FACE_pyTorch_small.t7\")\n",
    "vggFace.modules[31] = nn.View(1, 25088)\n",
    "print(vggFace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are 40 layers in total. The architecture can be divided into 5 Convolutional blocks followed by 2 fc layers and a classification layer. Each convolutional block consists of multiple Conv+ReLU layers followed by a pooling layer.Let us now send our pre-processed input image through the network i.e. we are going to do a forward pass through the pre-trained VGG-Face network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "input = torch.Tensor(final_image).unsqueeze(0)\n",
    "output = vggFace.forward(input)\n",
    "output = output.cpu().numpy()\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network has returned 2,622 entries corresponding to each test image. These values are the normalized log probabilities of our input face image belonging to each of the 2,622 celebrity IDs in the training set. So, in order to know which ID is the most likely (as per the network), we figure out the one which has the maximum probability.\n",
    "\n",
    "To get the name of the ID from the index, we will use the list of ID names present in **names.txt**. As you can guess, the order of names in the list is important and serves as a mapping between the index number and the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def getNameList(filePath):\n",
    "    names = []\n",
    "    with open(filePath) as f:\n",
    "        names = [ line.strip() for line in f ]\n",
    "    return names\n",
    "\n",
    "idNames = getNameList(DATA_ROOT+\"/names.txt\")\n",
    "print(\"predicted_label = \", idNames[np.argmax(output[0])], \"\\ntrue_label = \", label,\"\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. FEATURE EXTRACTION (VGG-Face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep features\n",
    "\n",
    "We have seen how to use the pre-trained net to make predictions (face recognition). The VGG-Face net can also be used as a fixed feature extractor for face images. We simply ignore the outputs of the classification layer and take the output of the last fc-layer instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggFeatures = vggFace.modules[35].output.cpu().numpy()\n",
    "print(vggFeatures.shape)\n",
    "print(np.linalg.norm(vggFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4096-d face features that we obtain in this fashion can be used with the classifiers that we used in the previous tutorials for a variety of face-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises\n",
    "\n",
    "5. Modify the code to support bacth mode of operation -- get top-k predictions for multiple face images at once.\n",
    "6. Along with the predicted ID name, also visualize the probability of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
