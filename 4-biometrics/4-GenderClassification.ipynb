{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Classification (Finetuning VGG-Face for other tasks)\n",
    "\n",
    "In this tutorial, we are going to fine-tune the pre-trained VGG-Face descriptors for the task of classifying the gender of a person from his/her face image. As we saw in the previous tutorial, the VGG-Face network has been trained to recognize 2,622 celebrity IDs. Also, we can ignore/chop-off the classification layer of the network and treat the output of the fc-layer as a representation for the input face.\n",
    "\n",
    "We are going to learn a new Softmax  classification layer on top of these fc layer features and train it to classify the gender of the input face image into one of the two classes -- \"male\" and \"female\". By the end of this tutorial, the would see that by minimal amount of fine-tuning, the pre-trained VGG-Face representations can be made useful for a variety of tasks that are different from the original task of face recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.serialization import load_lua\n",
    "from torch.legacy import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline \n",
    "plt.ion()\n",
    "\n",

    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "DATA_ROOT = \"/tmp/data/lab3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease-of-use, we have consolidated all the input image pre-processing related steps into a single function that is defined below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code to load and pre-process image for VGG-Face (made into a function)\n",
    "\n",
    "def loadImage(imgPath):\n",
    "    inputImg = cv2.imread(imgPath)\n",
    "\n",
    "    # re-scale the smaller dim (among width, height) to refSize\n",
    "    refSize, targetSize = 256, 224\n",
    "    imgRows, imgCols = inputImg.shape[0], inputImg.shape[1]\n",
    "    if imgCols < imgRows:\n",
    "        resizedImg = cv2.resize(inputImg, (refSize, int(refSize * imgRows / imgCols)))\n",
    "    else:\n",
    "        resizedImg = cv2.resize(inputImg, (int(refSize * imgCols / imgRows), refSize))\n",
    "\n",
    "    # center-crop\n",
    "    oH, oW = targetSize, targetSize\n",
    "    iH, iW = resizedImg.shape[0], resizedImg.shape[1]\n",
    "    anchorH, anchorW = int(math.ceil((iH - oH)/2)), int(math.ceil((iW - oW) / 2))\n",
    "    croppedImg = resizedImg[anchorH:anchorH+oH, anchorW:anchorW+oW]\n",
    "\n",
    "    # convert shape from (height, width, 3) to (3, width, height)\n",
    "    r, g, b = croppedImg[:, :, 0], croppedImg[:, :, 1], croppedImg[:, :, 2]\n",
    "    croppedImg = np.empty([3, croppedImg.shape[0], croppedImg.shape[1]])\n",
    "    croppedImg[0], croppedImg[1], croppedImg[2] = r, g, b\n",
    "\n",
    "    # subtract training mean\n",
    "    trainingMean = [129.1863, 104.7624, 93.5940]\n",
    "    for i in range(3): croppedImg[i] = croppedImg[i] - trainingMean[i]\n",
    "    return croppedImg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, as we saw towards the end of last experiment, we will make a function that extracts VGG-Face descriptors for a set of input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute pre-trained VGG-Face descriptors \n",
    "\n",
    "def getVggFeatures(imgPaths, preTrainedNet):\n",
    "    nImgs = len(imgPaths)\n",
    "    preTrainedNet.modules[31] = nn.View(nImgs, 25088)\n",
    "    if use_cuda:\n",
    "        preTrainedNet = preTrainedNet.cuda()\n",
    "    \n",
    "    batchInput = torch.Tensor(nImgs, 3, 224, 224)\n",
    "    for i in range(nImgs): batchInput[i] = torch.from_numpy(loadImage(imgPaths[i]))\n",
    "    \n",
    "    batchOutput = preTrainedNet.forward(batchInput.to(device))\n",
    "    return preTrainedNet.modules[35].output.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Network Architecture for Gender Classification\n",
    "Having defined the functions for pre-processing images and getting the pre-trained face descriptors, let us take a look at the simple network structure that we are going to train.\n",
    "\n",
    "We will just add 1 fc layer with a 2 class output and a log-softmax loss function over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network structure that we'll train for gender classification\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(4096, 2)\n",
    "        self.softmax = torch.nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The architecture is quite straightforward. We have one ```Linear(4096, 2)``` layer (fc-layer) which takes a 4096-d input and computes a vector that has two elements. So, our fc-layer can take the pre-trained descriptors as input and return a pair of values which would be the (unnormalized) likelihoods of the input image belonging to each of the two classes -- \"male\" and \"female\".\n",
    "\n",
    "- The fc-layer is followed by a ```LogSoftMax()``` layer which converts the unnormalized likelihoods returned by  the fc-layer into log-probabilities which would then be sent to our loss module. By training the parameters of the fc-layer to minimize the loss, our network will learn to map the pre-trained face descriptors to the correct gender class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We are going to use a subset of the CelebA dataset for our experiment. CelebA is a large-scale celebrity face attributes dataset. It consists of more than 200k celebrity face images, each having 40 binary face attribute annotations, with gender being one of them. We have selected a random subset of 200 face images -- 100 male and female faces each for the purpose of this experiment. Let us take a look at the distribution of the training and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_lua(DATA_ROOT+\"/gender_dataset/celeba-gender-dataset.t7\")\n",
    "print(\"# images in trainset = \", dataset['trainset'].size)\n",
    "print(\"# images in testset = \", dataset['testset'].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at some of the faces in the dataset, and their asosciated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgPath = DATA_ROOT+\"/gender_dataset/\" + dataset['trainset'].imgPaths[0]\n",
    "imgLabel = dataset['trainset'].labels[0]\n",
    "\n",
    "dispImg = cv2.imread(imgPath)\n",
    "plt.subplot(121)\n",
    "plt.imshow(cv2.cvtColor(dispImg, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"label = \"+imgLabel)\n",
    "\n",
    "imgPath = DATA_ROOT+\"/gender_dataset/\" + dataset['trainset'].imgPaths[100]\n",
    "imgLabel = dataset['trainset'].labels[100]\n",
    "dispImg = cv2.imread(imgPath)\n",
    "plt.subplot(122)\n",
    "plt.imshow(cv2.cvtColor(dispImg, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"label = \"+imgLabel)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`label=1` stands for \"male\" whereas `label=0` stands for female. Let us also take a look at an example of a female face image in our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 160 training and 40 test images with each set having an equal distribution of male and female faces. Participants are encouraged to modify the code and view some of the different face images (and their asosciated labels) in the training as well as test sets.\n",
    "\n",
    "### Training\n",
    "Now that we have fixed the network architecture as well as the dataset, let us move on to training our network on the dataset. We are going to use a combination of **`LogSoftMax` + `NLLLoss`** in PyTorch to train the network. Let us take a look at the various steps involved.\n",
    "\n",
    "- First of all, we fix the seed of the various random number generators that our code uses. This is to ensure that all are experimental results are reproducible.\n",
    "\n",
    "Having fixed the seed, our network parameters will always be initialized with the same random values every time we run the experiments. Also, the sampling of the mini-batch during training will also follow the same order in different runs of the experiment. You can verify this fact by commenting out the first 2 lines in the next block and comparing the value of the training loss in successive runs of the experiment.\n",
    "\n",
    "- Next, we load the pre-trained VGG-Face model and dataset. We also initialize our network architecture, loss module and certain other training parameters such as number of epochs to train and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the seeds of random number generators\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# load the dataset and the pre-trained network\n",
    "vggFace = load_lua(DATA_ROOT+\"/VGG_FACE_pyTorch_small.t7\")\n",
    "dataset = load_lua(DATA_ROOT+\"/gender_dataset/celeba-gender-dataset.t7\")\n",
    "\n",
    "# initialize the net, loss and optimizer (SGD)\n",
    "net = Net()\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.00005, momentum=0.9, weight_decay=0.0005)\n",
    "nEpochs, batchSize = 4, 10\n",
    "\n",
    "net = net.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we jump into the training code, we would first define a function that lets us evaluate how good our model is performing on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to test the performance of our model on a separate test set\n",
    "def evaluate(net, dataset):\n",
    "    correctPreds = 0.0\n",
    "    for startIdx in range(0, dataset['testset'].size, batchSize):\n",
    "        endIdx = min(startIdx + batchSize - 1, dataset['testset'].size - 1)\n",
    "        size = (endIdx - startIdx + 1)\n",
    "        \n",
    "        ## make a batch\n",
    "        batchInput, batchLabel = torch.Tensor(size, 4096), torch.LongTensor(size)\n",
    "        batchImgPaths = []\n",
    "        for offset in range(size):\n",
    "            imgPath = DATA_ROOT+\"/gender_dataset/\" + dataset['testset'].imgPaths[startIdx+offset]\n",
    "            batchImgPaths.append(imgPath)\n",
    "            label = dataset['testset'].labels[startIdx+offset]\n",
    "            batchLabel[offset] = int(label)\n",
    "        \n",
    "        batchInput = getVggFeatures(batchImgPaths, vggFace)\n",
    "\n",
    "        ## feed forward the features through the network\n",
    "        batchOutput = net(batchInput.to(device))\n",
    "        batchOutput, batchLabel = batchOutput.data.cpu().numpy(), batchLabel.numpy()\n",
    "        predictions = np.argmax(batchOutput, 1)\n",
    "        correctPreds += np.sum(predictions == batchLabel)\n",
    "    return correctPreds / dataset['testset'].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the performance of our (un-trained) network on the test set by calling the `evaluate()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy (before training) = \", evaluate(net, dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the overall classification accuracies are quite low. At the moment, the network might still perform better than a random assignment of classes. This is to be expected because the parameters of the network are randomly initialized and no training has been done till now. Also, it is a binary classification problem so even if we classify all images as 1 class, we still get 50% accuracy.\n",
    "\n",
    "Let us see the code for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "\n",
    "for epochCtr in range(nEpochs):\n",
    "    \n",
    "    shuffle = np.random.permutation(dataset['trainset'].size)\n",
    "    runningLoss, iterCnt = 0.0, 0\n",
    "    for startIdx in range(0, dataset['trainset'].size, batchSize):\n",
    "        endIdx = min(startIdx + batchSize - 1, dataset['trainset'].size-1)\n",
    "        size = (endIdx - startIdx + 1)\n",
    "    \n",
    "        ## make a batch\n",
    "        batchInput, batchLabel = torch.Tensor(size, 4096), torch.LongTensor(size)\n",
    "        batchImgPaths = []\n",
    "        for offset in range(size):\n",
    "            imgPath = DATA_ROOT+\"/gender_dataset/\" + dataset['trainset'].imgPaths[shuffle[startIdx+offset]]\n",
    "            batchImgPaths.append(imgPath)\n",
    "            label = dataset['trainset'].labels[shuffle[startIdx+offset]]\n",
    "            batchLabel[offset] = int(label) \n",
    "        \n",
    "        ## get features for the batch images \n",
    "        batchInput = getVggFeatures(batchImgPaths, vggFace)\n",
    "        \n",
    "        ## feed forward the features through the network and calculate loss\n",
    "        batchOutput = net(batchInput.to(device))\n",
    "        loss = criterion(batchOutput, batchLabel.to(device))\n",
    "        \n",
    "        ## backpropagate and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        runningLoss += loss.item()\n",
    "        iterCnt += 1\n",
    "        if iterCnt % 4 == 0:\n",
    "            print(\"epoch=\", (epochCtr+1), \"/\", nEpochs, \", iter=\", iterCnt, \", loss=\", runningLoss/4)\n",
    "            runningLoss = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained the network, let us see if the performance on the test set has improved over what we got prior to training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy (after training) = \", evaluate(net, dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is a significant jump in performance after training for just 1 epoch (16 iterations).\n",
    "\n",
    "Let us see our network in action as it classifies individual images from the test set. Recall that `label=1` denotes \"male\" whereas `label=0` denotes female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgPath = DATA_ROOT+\"/gender_dataset/\" + dataset['testset'].imgPaths[39]\n",
    "dispImg = mpimg.imread(imgPath)\n",
    "imgPlot = plt.imshow(dispImg)\n",
    "imgLabel = dataset['testset'].labels[39]\n",
    "print(\"true label = \", imgLabel)\n",
    "\n",
    "features = getVggFeatures([imgPath], vggFace)\n",
    "\n",
    "output = net(features.to(device)).data.cpu().numpy()\n",
    "    \n",
    "print(\"predicted label = \", np.argmax(output, 1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercises\n",
    "7. Can you compute and display accuracies for male and female separately? Accuracies here refer to both precision as well as recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
